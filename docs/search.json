[
  {
    "objectID": "src/fine-tuning_final.html",
    "href": "src/fine-tuning_final.html",
    "title": "Fine-Tuning Kodis",
    "section": "",
    "text": "I spent a lot of time looking at CRAFT under the hood this week to see possible modifications. This initial “exploratory” fine-tuning notebook ended up necessitating using cloud compute(used deepnote) for fine-tuning (GP4 -L4 $1.56/hr) to get some initial results, as each run for fine-tuning one model with training size (1264) was taking 40 min to train on my machine. To really improve results, I would look into setting up full end-to-end with python scripts to manage experiment runs better. I did a couple this week (1-3 for each model variant) in this notebook, and there is still randomness that is not reproducible across experiment runs. There is a lot fo room to modify the “model_config” for CRAFT and perhaps do dynamic adjustments to the learning rate for each batch, as I had issues with overfitting, but this notebook is not generalizble to test at this level.\nWhat does “doing better” mean to know if we should continue? Pivot to analysis with self-report scores?\n\nIt is pretty ineffecient to continue workin in jupyter notebook for fine-tuning as this is no way a robust training pipeline, but I wanted to touch base to see if the above would make sense to continue trying\n\nThe few downsampling training runs had high accuracy variablitiy (27% to 80%)\nThere are some clear upward patterns in the prediciton scores broken out by success/impasse’s emerging\ndownsampling does affect the accuracy quite a bit\nAUC/PR curves look promising. I also chose the best threshold and plotted metric comparisons as well as the Confusion Matrix across each variant."
  },
  {
    "objectID": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighteddownsampled-variants",
    "href": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighteddownsampled-variants",
    "title": "Fine-Tuning Kodis",
    "section": "Ground Model Performance Comparison Across Default, Weighted,Downsampled Variants",
    "text": "Ground Model Performance Comparison Across Default, Weighted,Downsampled Variants\n\n\nCode\ncorpora_info_ground = [\n    (\n        \"GROUND_DEFAULT\",\n        no_samp   [\"ground_corpus\"],\n        no_samp   [\"ground_metrics\"],\n        no_samp   [\"ground_df\"],\n        no_samp   [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_WEIGHTED\",\n        wt        [\"ground_corpus\"],\n        wt        [\"ground_metrics\"],\n        wt        [\"ground_df\"],\n        wt        [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_DOWNSAMPLED\",\n        down      [\"ground_corpus\"],\n        down      [\"ground_metrics\"],\n        down      [\"ground_df\"],\n        down      [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nbest_thresholds_ground, best_metrics_ground, best_corpora_ground = compare_craft_models(corpora_info_ground)\n\n\n== Avg. Conversation Length ==\n  GROUND_DEFAULT        train=13.0  test=13.1\n  GROUND_WEIGHTED       train=13.1  test=13.1\n  GROUND_DOWNSAMPLED    train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nGROUND_DEFAULT\n0.303318\n0.198910\n1.000000\n0.842407\n0.331818\n\n\nGROUND_WEIGHTED\n0.175355\n0.173397\n1.000000\n0.997135\n0.295547\n\n\nGROUND_DOWNSAMPLED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGROUND_DEFAULT       best thr=0.541, TPR=1.000, FPR=0.000, J=1.000\nGROUND_WEIGHTED      best thr=0.575, TPR=0.699, FPR=0.072, J=0.627\nGROUND_DOWNSAMPLED   best thr=0.581, TPR=0.671, FPR=0.112, J=0.559\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nGROUND_DEFAULT_acc\nGROUND_WEIGHTED_acc\nGROUND_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nGROUND_DEFAULT_avg_prob\nGROUND_WEIGHTED_avg_prob\nGROUND_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.303318\n0.175355\n0.172986\n0.443128\n0.519425\n0.560313\n0.56887\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Performance Comparison\n\n\nCode\ncompare_best_models(best_thresholds_ground, best_metrics_ground, best_corpora_ground)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nGROUND_DEFAULT\n1.000000\n1.000000\n1.000000\n0.000000\n1.000000\n0.541032\n\n\nGROUND_WEIGHTED\n0.827014\n0.000000\n0.000000\n0.000000\n0.000000\n0.575249\n\n\nGROUND_DOWNSAMPLED\n0.850711\n0.556818\n0.671233\n0.111748\n0.608696\n0.580783\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/root/venv/lib/python3.11/site-packages/numpy/lib/_histograms_impl.py:902: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-last-utterance-model-performance-comparison-across-default-weighteddownsampled-variants",
    "href": "src/fine-tuning_final.html#no-last-utterance-model-performance-comparison-across-default-weighteddownsampled-variants",
    "title": "Fine-Tuning Kodis",
    "section": "No Last Utterance Model Performance Comparison Across Default, Weighted,Downsampled Variants",
    "text": "No Last Utterance Model Performance Comparison Across Default, Weighted,Downsampled Variants\n\n\nCode\ncorpora_info_no_last = [\n    (\n        \"NO_LAST_DEFAULT\",\n        no_samp[\"no_last_corpus\"],\n        no_samp[\"no_last_metrics\"],\n        no_samp[\"no_last_df\"],\n        no_samp[\"no_last_horizon\"],\n    ),\n    (\n        \"NO_LAST_WEIGHTED\",\n        wt[\"no_last_corpus\"],\n        wt[\"no_last_metrics\"],\n        wt[\"no_last_df\"],\n        wt[\"no_last_horizon\"],\n    ),\n    (\n        \"NO_LAST_DOWNSAMPLED\",\n        down[\"no_last_corpus\"],\n        down[\"no_last_metrics\"],\n        down[\"no_last_df\"],\n        down[\"no_last_horizon\"],\n    ),\n    (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nbest_thresholds_no_last, best_metrics_no_last, best_corpora_no_last = compare_craft_models(corpora_info_no_last)\n\n\n== Avg. Conversation Length ==\n  NO_LAST_DEFAULT       train=13.0  test=13.1\n  NO_LAST_WEIGHTED      train=13.1  test=13.1\n  NO_LAST_DOWNSAMPLED   train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nNO_LAST_DEFAULT\n0.175355\n0.173397\n1.000000\n0.997135\n0.295547\n\n\nNO_LAST_WEIGHTED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nNO_LAST_DOWNSAMPLED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNO_LAST_DEFAULT      best thr=0.555, TPR=0.699, FPR=0.499, J=0.200\nNO_LAST_WEIGHTED     best thr=0.577, TPR=0.630, FPR=0.441, J=0.189\nNO_LAST_DOWNSAMPLED  best thr=0.560, TPR=0.890, FPR=0.665, J=0.226\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nNO_LAST_DEFAULT_acc\nNO_LAST_WEIGHTED_acc\nNO_LAST_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nNO_LAST_DEFAULT_avg_prob\nNO_LAST_WEIGHTED_avg_prob\nNO_LAST_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.175355\n0.172986\n0.172986\n0.443128\n0.557905\n0.574563\n0.568332\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Performance Comparison\n\n\nCode\ncompare_best_models(best_thresholds_no_last, best_metrics_no_last, best_corpora_no_last)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nNO_LAST_DEFAULT\n0.535545\n0.226667\n0.698630\n0.498567\n0.342282\n0.555385\n\n\nNO_LAST_WEIGHTED\n0.827014\n0.000000\n0.000000\n0.000000\n0.000000\n0.576696\n\n\nNO_LAST_DOWNSAMPLED\n0.431280\n0.218855\n0.890411\n0.664756\n0.351351\n0.559719\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/root/venv/lib/python3.11/site-packages/numpy/lib/_histograms_impl.py:902: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tuning Kodis",
    "section": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\n\ncorpora_info_no_subm = [\n    (\n        \"NO_LAST_SUBMIT_DEFAULT\",\n        no_samp[\"no_subm_corpus\"],\n        no_samp[\"no_subm_metrics\"],\n        no_samp[\"no_subm_df\"],\n        no_samp[\"no_subm_horizon\"],\n    ),\n    (\n        \"NO_LAST_SUBMIT_WEIGHTED\",\n        wt[\"no_subm_corpus\"],\n        wt[\"no_subm_metrics\"],\n        wt[\"no_subm_df\"],\n        wt[\"no_subm_horizon\"],\n    ),\n    (\n        \"NO_LAST_SUBMIT_DOWNSAMPLED\",\n        down[\"no_subm_corpus\"],\n        down[\"no_subm_metrics\"],\n        down[\"no_subm_df\"],\n        down[\"no_subm_horizon\"],\n    ), (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nno_last_submit_thresholds, no_last_submit_metrics, no_last_submit_corpora = compare_craft_models(corpora_info_no_subm)\n\n\n== Avg. Conversation Length ==\n  NO_LAST_SUBMIT_DEFAULT  train=13.0  test=13.1\n  NO_LAST_SUBMIT_WEIGHTED  train=13.1  test=13.1\n  NO_LAST_SUBMIT_DOWNSAMPLED  train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT\n0.670616\n0.318681\n0.794521\n0.355301\n0.454902\n\n\nNO_LAST_SUBMIT_WEIGHTED\n0.777251\n0.370370\n0.410959\n0.146132\n0.389610\n\n\nNO_LAST_SUBMIT_DOWNSAMPLED\n0.255924\n0.188630\n1.000000\n0.899713\n0.317391\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT best thr=0.521, TPR=0.781, FPR=0.321, J=0.460\nNO_LAST_SUBMIT_WEIGHTED best thr=0.497, TPR=0.699, FPR=0.252, J=0.446\nNO_LAST_SUBMIT_DOWNSAMPLED best thr=0.758, TPR=0.753, FPR=0.232, J=0.521\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT_acc\nNO_LAST_SUBMIT_WEIGHTED_acc\nNO_LAST_SUBMIT_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nNO_LAST_SUBMIT_DEFAULT_avg_prob\nNO_LAST_SUBMIT_WEIGHTED_avg_prob\nNO_LAST_SUBMIT_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.670616\n0.777251\n0.255924\n0.443128\n0.508492\n0.495041\n0.677392\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Model Performance Comparison\n\n\nCode\ncompare_best_models(no_last_submit_thresholds, no_last_submit_metrics, no_last_submit_corpora)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT\n0.696682\n0.337278\n0.780822\n0.320917\n0.471074\n0.521406\n\n\nNO_LAST_SUBMIT_WEIGHTED\n0.739336\n0.366906\n0.698630\n0.252149\n0.481132\n0.497068\n\n\nNO_LAST_SUBMIT_DOWNSAMPLED\n0.765403\n0.404412\n0.753425\n0.232092\n0.526316\n0.757998\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697"
  },
  {
    "objectID": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tuning Kodis",
    "section": "Ground Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "Ground Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"ground_corpus\"], name = \"Ground Default\")\nplot_position_score_evolution_by_outcome(down[\"ground_corpus\"], name = \"Ground Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"ground_corpus\"], name = \"Ground Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-last-utt-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#no-last-utt-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tuning Kodis",
    "section": "No Last Utt Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last Utt Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"no_last_corpus\"], name = \":No Last Default\")\nplot_position_score_evolution_by_outcome(down[\"no_last_corpus\"], name = \":No Last Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"no_last_corpus\"], name = \":No Last Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants-1",
    "href": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants-1",
    "title": "Fine-Tuning Kodis",
    "section": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"no_subm_corpus\"], name = \":No Last/No Submit Default\")\nplot_position_score_evolution_by_outcome(down[\"no_subm_corpus\"], name = \":No Last/No Submit Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"no_subm_corpus\"], name = \":No Last/No Submit Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  },
  {
    "objectID": "src/fine-tuning_analysis.html",
    "href": "src/fine-tuning_analysis.html",
    "title": "Fine-Tuning Kodis Analysis",
    "section": "",
    "text": "Code\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport pprint as pp\nfrom IPython.display import display\nimport torch\nimport pickle\nimport random\nfrom pathlib import Path \nimport json\nimport re\nfrom collections import defaultdict\nfrom scipy.stats import entropy\nimport ast\nfrom collections import defaultdict\nimport importlib\nimport torch\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    roc_curve,\n    roc_auc_score,\n    precision_recall_curve,\n    average_precision_score,\n)\nfrom modules.DataPreprocesser import DataPreprocesser\nfrom modules import CorpusUtils as corp\n#Convokit Imports\nfrom convokit.forecaster.CRAFTModel import CRAFTModel\nfrom convokit.forecaster.forecaster import Forecaster\nfrom convokit import download, Corpus\nfrom convokit import Corpus, Speaker, Utterance, Conversation\nfrom functools import partial\nfrom convokit.convokitConfig import ConvoKitConfig\nCode\ndownpath =Path(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/fine_tuning_results/downsampled_run/corpus_kodis_ground_downsampled\")\ndefaultpath =Path(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/fine_tuning_results/nosampling_run/corpus_kodis_ground_default\")\nweights_path = Path(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/fine_tuning_results/weighted_run/corpus_kodis_ground_weighted_loss\")\n\ncorpus_down = Corpus(filename=downpath)\ncorpus_default = Corpus(filename=defaultpath)\ncorpus_weighted = Corpus(filename=weights_path)"
  },
  {
    "objectID": "src/fine-tuning_analysis.html#improvement-considerations",
    "href": "src/fine-tuning_analysis.html#improvement-considerations",
    "title": "Fine-Tuning Kodis Analysis",
    "section": "Improvement Considerations:",
    "text": "Improvement Considerations:\n\nIncluding Learning‑rate scheduling or early stopping to stop training early when it converges to prevent overfitting/underfitting\nfull-end to end training with testing model_config parameters to improve metrics"
  },
  {
    "objectID": "src/fine-tuning_analysis.html#design-decisions",
    "href": "src/fine-tuning_analysis.html#design-decisions",
    "title": "Fine-Tuning Kodis Analysis",
    "section": "Design Decisions:",
    "text": "Design Decisions:\n\nDo we include submit agreements or no? Lose out on potential dispute dynamics with chains of submit agreements\ntest on ground kodis conversations?\nRevalant Analysis considerations?\nsentiment granularity: comparing utterance level predicitons with conversational-leel self-report scores\npredicting SVI (self, fairness, outcome, relationships). Predict Frustration?"
  },
  {
    "objectID": "src/fine-tuning_analysis.html#todo",
    "href": "src/fine-tuning_analysis.html#todo",
    "title": "Fine-Tuning Kodis Analysis",
    "section": "TODO:",
    "text": "TODO:\n\nGo over CARC tutortial + set up GPU\nPredict SVI (relationships, frustration, outcome, impasse)"
  }
]