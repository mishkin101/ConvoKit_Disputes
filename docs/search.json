[
  {
    "objectID": "src/Forcaster_Explorer.html",
    "href": "src/Forcaster_Explorer.html",
    "title": "Forecaster Prediction Performance",
    "section": "",
    "text": "Code\n%%capture\nfrom tqdm import tqdm\nfrom convokit import Corpus, Speaker, Utterance\nfrom collections import defaultdict\nimport pandas as pd\nimport pprint as pp\nfrom IPython.display import display\nfrom modules.DataPreprocesser import DataPreprocesser\nfrom modules import CorpusUtils as corp\nimport torch\nimport pickle\n\n# Add the src directory to the path\nimport sys\nimport os\n\n# Add the src\nsys.path.append(os.path.abspath(\".\"))\nimport import_ipynb\n\n#Convokit Imports\nfrom convokit.forecaster.CRAFTModel import CRAFTModel\nfrom convokit.forecaster.forecaster import Forecaster\n\n\n\nTodo:\n\nFighting words for impasse and success disputes\nA paper from paper from Jonathan P. Cheng (Horizon paper) that preceeeds the CRAFT model defines pragmatic devices—such as politeness strategies and rhetorical prompts—used to start a conversation, and analyze their relation to its future trajectory.\n\nConvokit has a notebook (very recent-2025) that creates Prompt Types/politeness strategy labels for paired utterances to make derailement predicitons\n\nPlot utterances frequenices across context length dimension (averaged across all conversations)\ndownsample KODIS (balanced) -&gt; check performance\nfine-tune wiki on KODIS -&gt; check performance\nresult “leakage” from submission diagloge utterances?\n\n\n\nPerformance Considerations:\n\nIs the relative horizon (normalizing for conversation length) for forcasting derailement in KODIS disputes similar to the horizon for WIKI and CMV test dialogues?\nHow early is “good enough” for us?\nOur tokenized utterances are very short (avg.len 23) vs 84/123. Is our in-vitro dispute comprable to other dispute datasets or are these confounding factors influencing the length of negotiation utterances? Shorter utterances influence CRAFT’s performance significantly?\n\n\n\nCreating KODIS Corpus Object\n\nCreate Corpus\n\n\nCode\nfilepath = \"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/preprocessed_dyads.csv\"\nfinal_data = DataPreprocesser(filepath)\ntest_corp= corp.corpusBuilder(final_data)\n\n\n2107\n\n\n/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/src/modules/DataPreprocesser.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1702723625' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  self.utterancesDF.loc[13988, 'timestamp']= '1702723625'\n27498it [00:00, 54368.26it/s]\n\n\n\n\nCode\ntest_corp.print_summary_stats()\n\n\nNumber of Speakers: 4214\nNumber of Utterances: 27498\nNumber of Conversations: 2107\n\n\n\n\nAdd conversation lengths as conversation metadata\n\n1 is “impasse”\n0 is “success”\n\n\n\nCode\nfor convo in test_corp.iter_conversations():\n    convo_len = len(convo.get_utterance_ids())  # Count utterances in the conversation\n    convo.add_meta(\"convo_len\", convo_len)      # Store as conversation-level metadata\n    some_convo = test_corp.get_conversation(\"utt0_con0\")\nprint(\"Length of conversation:\", some_convo.retrieve_meta(\"convo_len\"))\n\n\nLength of conversation: 14\n\n\n\n\nAdd Conversation Labels from Final Pre-processed Dataframe as conversation metadata\n\n\nCode\nfor idx, row in final_data.getDataframe().iterrows():\n    convo_id = f\"utt0_con{idx}\"  # generate conversation_id format from index\n    label = row[\"dispute_outcome\"]  # update if your label column is named differently\n    if convo_id in test_corp.conversations:\n        test_corp.get_conversation(convo_id).meta[\"label\"] = label\n\n\n\n\n\nCreating Forecaster and Model Objects\n\nCMV Model and Forecaster Object for KODIS\n\n\nCode\n%%capture \nmodel2 = CRAFTModel(\n    initial_weights= \"craft-cmv-finetuned\",  # or \"craft-wiki-finetuned\"\n    torch_device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n\n\n\nCode\n# Use 'label' because we added it to conversation.meta[\"label\"]\nforecaster2 = Forecaster(\n    forecaster_model=model2,\n    labeler=\"label\",  # uses conversation.meta[\"label\"]\n    forecast_attribute_name=\"prediction\",\n    forecast_prob_attribute_name=\"pred_score\"\n)\n\n\n\n\nWiki Model and Forecaster Object for KODIS\n\n\nCode\n%%capture \nmodel = CRAFTModel(\n    initial_weights= \"craft-wiki-finetuned\",  # or \"craft-wiki-finetuned\"\n    torch_device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n\n\n\nCode\n# Use 'label' because we added it to conversation.meta[\"label\"]\nforecaster = Forecaster(\n    forecaster_model=model,\n    labeler=\"label\",  # uses conversation.meta[\"label\"]\n    forecast_attribute_name=\"prediction\",\n    forecast_prob_attribute_name=\"pred_score\"\n)\n\n\n\n\nForcaster Object for CMV and WIKI\n\n\nCode\nforecaster_cmv = Forecaster(\n    forecaster_model=model2,\n    labeler=\"has_removed_comment\",  # uses conversation.meta[\"label\"]\n    forecast_attribute_name=\"prediction\",\n    forecast_prob_attribute_name=\"pred_score\"\n)\n\nforecaster_wiki = Forecaster(\n    forecaster_model=model,\n    labeler=\"conversation_has_personal_attack\",  # uses conversation.meta[\"label\"]\n    forecast_attribute_name=\"prediction\",\n    forecast_prob_attribute_name=\"pred_score\"\n)\n\n\n\n\n\nModel Weight Info\n\ncraft‑pretrained\n\nContains only the utterance and context encoder layers pre‑trained on the CGA‑Wikipedia or CGA-CMV data (via next‑comment prediction), but its classifier head (the SingleTargetClf) is still at its random initialization.\nIntended as a starting point if you want to fine‑tune CRAFT on your own conversational data (you’ll call fit to learn the classifier weights).\n\n\n\ncraft‑finetuned (used)\n\nBuilds on the above by having already fine‑tuned the entire network (including the classifier head).\nReady for inference only—you can call transform immediately and get sensible forecasts without any further training.\n\n\n\nCode\ndisplay(model._config)\n\n\n{'dropout': 0.1,\n 'batch_size': 64,\n 'clip': 50.0,\n 'learning_rate': 1e-05,\n 'print_every': 10,\n 'finetune_epochs': 30,\n 'validation_size': 0.2}\n\n\n\n\n\nComparing fine-tuned Models on KODIS Corpus\n\nRunning predictions on both fine-tuned wiki and cmv CRAFT models temporally on all utterances\n\n\nCode\n%%capture \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n\n\n# corpusBuilder should return a ConvoKit Corpus\ncorp1 = corp.corpusBuilder(final_data)\ncorp2 = corp.corpusBuilder(final_data)\n\n# Assign conversation-level labels\nfor idx, row in final_data.getDataframe().iterrows():\n    convo_id = f\"utt0_con{idx}\"\n    label = row[\"dispute_outcome\"]\n    if convo_id in corp1.conversations:\n        corp1.get_conversation(convo_id).meta[\"label\"] = label\n    if convo_id in corp2.conversations:\n        corp2.get_conversation(convo_id).meta[\"label\"] = label\n\n# # 2. Initialize two forecasters with different CRAFT weights\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# craft_wiki = CRAFTModel(\"craft-wiki-finetuned\", torch_device=device)\n# craft_cmv  = CRAFTModel(\"craft-cmv-finetuned\",  torch_device=device)\n# forecaster1 = Forecaster(craft_wiki, \"label\")\n# forecaster2 = Forecaster(craft_cmv,  \"label\")\n\n\n\n# 4. Transform both corpora\ncorp1 = forecaster.transform(corp1)\ncorp2 = forecaster2.transform(corp2)\n\n\n\n\nGetting average derailement prediction and frequency of forecast probabilities across all utterances\n\nPrediciton is per utterance on surface, but CRAFT model behind-the-scenes makes predicitons on encoded contexts which contain the conversation history up to current utterance.\nSee Predictor Class in runnery.py for CRAFT model\nWe get the frequency of KODIS utterances across 50 probabilities [0, .02, .04, …,1] binned from the predicited probabilities of each utterance by the fine-tuned cmv and fine-tuned wiki model\n\n\n\nInsights\n\ncraft-wiki tends to\n\nTODO Make 3D plot, add another dimension for the length of current context (collapse by coversation by averaging over all convos that have that context length)\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n\n# 1) extract the utterance‑level DataFrames from the two corpora\nutt_df1 = corp1.get_utterances_dataframe()\nutt_df2 = corp2.get_utterances_dataframe()\n\n\n# 5. Keep only the forecast columns and drop NA\nforecast_df1 = utt_df1[['meta.prediction', 'meta.pred_score']].dropna()\nforecast_df2 = utt_df2[['meta.prediction', 'meta.pred_score']].dropna()\n\n# 6. Summary statistics\nsummary = pd.DataFrame({\n    'mean_prob':  [forecast_df1['meta.pred_score'].mean(),  forecast_df2['meta.pred_score'].mean()],\n    'std_prob':   [forecast_df1['meta.pred_score'].std(),   forecast_df2['meta.pred_score'].std()],\n    'median_prob':[forecast_df1['meta.pred_score'].median(),forecast_df2['meta.pred_score'].median()],\n}, index=['corp-wiki', 'corp-cmv'])\nprint(\"Summary statistics for forecast probabilities:\")\nprint(summary)\n\n# 7. Plot histograms\nplt.figure(figsize=(8,4))\nplt.hist(forecast_df1['meta.pred_score'], bins=50, alpha=0.6, label='craft-wiki')\nplt.hist(forecast_df2['meta.pred_score'], bins=50, alpha=0.6, label='craft-cmv')\nplt.xlabel('Forecast Derailment probability')\nplt.ylabel('Number of Utterances')\nplt.legend()\nplt.show()\n\n\nSummary statistics for forecast probabilities:\n           mean_prob  std_prob  median_prob\ncorp-wiki   0.440724  0.267856     0.381885\ncorp-cmv    0.541044  0.271951     0.528305\n\n\n\n\n\n\n\n\n\n\n\nConversation-level AUC and PR Curves\nto create these plots, I looked at Forecasters code for aggregating utternce-levele metrics on a conversation level where:\n\nnp.max(forecast_scores) is highest probability the model ever assigned to any utterance in this conversation\nnp.max(forecasts) is if the model ever cross its decision threshold and predict 1 (derailement) for thsi conversation\n\n\n\nInsights\n\nBaseline positive class rate (impasse dispute) for KODIS is 17.6% out of current 2107 disputes.\n\nPR Curve\n\nBottoms out at baseline impasse rate with lower decision thresholds. Essesntially, as I lower decision threshold (less confident about classifying impasse) -&gt; it mispredicts impasse a lot more for successful conversations since higher recall -&gt; less chance of 0(success) outcome. Therefore, model is as good as random classifier as it predicts the baseline derailment rate from the sample. Probably due to high impassee/success class imbalance.\nwiki model outperforms cmv model acrosss all decision thresholds, so it does better overall with getting successful disputes right\n\nAUC Curve\n\nAUC still looks high for both bc of the class imbalance for derailement disputes I think– so this may be misleading currently\n\nneed to downsample and check if we do better than random\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\nimport numpy as np\n\ndef get_conv_level_scores(corpus):\n    # 1) grab all utterances with a forecast\n    utt = (corpus\n            .get_utterances_dataframe()[['conversation_id','meta.pred_score']]\n            .dropna())\n    conv_scores = utt.groupby('conversation_id')['meta.pred_score'].max()\n    conv_df = corpus.get_conversations_dataframe()\n    conv_scores = conv_scores.reindex(conv_df.index, fill_value=0)\n    y_true = conv_df['meta.label'].astype(int)\n    y_scores = conv_scores.values\n    return y_true, y_scores\n\ndef print_baseline_accuracy(corpus, label_field=\"label\"):\n    # pull out the conversation‑level labels\n    conv_df = corpus.get_conversations_dataframe()[[f\"meta.{label_field}\"]]\n    \n    # count each class\n    counts = conv_df[\"meta.label\"].value_counts()\n    min_label = counts.idxmin()    \n    min_count = counts.min()      \n    total     = counts.sum()\n    baseline_acc = min_count / total\n    print(f\"Baseline accuracy (derailement): {baseline_acc:.3f}\")\n    \n\n\nconv_df = corp1.get_conversations_dataframe()[[f\"meta.label\"]]\ndisplay(conv_df[\"meta.label\"].value_counts())\ny1, s1 = get_conv_level_scores(corp1)\ny2, s2 = get_conv_level_scores(corp2)\n\npositive_rate = np.mean(y1)\nprint(f\"Positive class rate (Derailement): {positive_rate:.2%}\")\nprint_baseline_accuracy(corp1)\n\n# compute ROC and PR metrics\nfpr1, tpr1, _    = roc_curve(y1, s1)\nroc_auc1         = auc(fpr1, tpr1)\nfpr2, tpr2, _    = roc_curve(y2, s2)\nroc_auc2         = auc(fpr2, tpr2)\n\nprec1, rec1, _   = precision_recall_curve(y1, s1)\npr_auc1          = auc(rec1, prec1)\nprec2, rec2, _   = precision_recall_curve(y2, s2)\npr_auc2          = auc(rec2, prec2)\n\n# plot side‑by‑side\nfig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12,5))\n\n# ROC panel\nax_roc.plot(fpr1, tpr1, label=f'corp-wiki (AUC={roc_auc1:.3f})')\nax_roc.plot(fpr2, tpr2, label=f'corp-cmv (AUC={roc_auc2:.3f})')\nax_roc.set_xlabel('False Positive Rate')\nax_roc.set_ylabel('True Positive Rate')\nax_roc.set_title('ROC Curves')\nax_roc.legend()\n\n# PR panel\nax_pr.plot(rec1, prec1, label=f'corp-wiki (AUC={pr_auc1:.3f})')\nax_pr.plot(rec2, prec2, label=f'corp-cmv (AUC={pr_auc2:.3f})')\nax_pr.set_xlabel('Recall')\nax_pr.set_ylabel('Precision')\nax_pr.set_title('Precision–Recall Curves')\nax_pr.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nmeta.label\n0    1736\n1     371\nName: count, dtype: int64\n\n\nPositive class rate (Derailement): 17.61%\nBaseline accuracy (derailement): 0.176\n\n\n\n\n\n\n\n\n\n\n\nComparing forcaster summaries for both fine-tuned wiki and cmv CRAFT models predictions\n\nRan using Forecasters summarize function for conversation-level statistics\nboth reddit and cmv have Utterance-level labels for derailed comment as well as classify derailment on a conversation level if the conversation contains a derailement comment.\n\n\n\nInsights\nForecast Horizon: “How early can we detect derailement?”\n\nThis measures the number of utterances after the derailed utterance the model predicted in a coversation.\nSince our dataset has an average length of 13.5 utterances, the forecast horizon on average is 9.72 and 10 respecitvely for the WIKI and cmv models, meaning derailement is forecast towards the end of a KODIS dispute.\n\nConversation Metrics - On a conversation-level, accuracy of predicting derailement is pretty low and F1 scores are low for both fine-tuned models\n\nAccuracy: The model mispredicts derailements most of the time (for success class mainly) - out of 2017 disputes, 45% and 24% were wrongly flagged as derailed. -&gt; low Accuracy\nRecall: Models are very sensitive to derailement events and for all actual derailed disputes, it correctly flags them -&gt; high Recall\nFPR: Since there are many successul disputes (majority class), the models flag a lot of them as derailed -&gt; high FPR\n\nCalibration Curves\n\nboth models underperform\n\nThe above is why the “good” AUC curve is misleading I think\n\n\nCode\n%%capture\nimport numpy as np\nimport matplotlib.pyplot as plt\nhorizon_kwiki = forecaster._draw_horizon_plot(corp1)\nhorizon_kcmv  = forecaster2._draw_horizon_plot(corp2)\nconv_kwiki, metrics1_kwiki = forecaster.summarize(corp1)\nconv_kcmv, metrics2_kcmv = forecaster2.summarize(corp2)\n\n\n\n\nCode\nimport pandas as pd\nfrom IPython.display import display\n\nlengths = [\n    len(convo.get_utterance_ids())\n    for convo in corp2.iter_conversations()\n]\navg_len = sum(lengths) / len(lengths)\nprint(f\"Average conversation length in KODIS is: {avg_len:.2f} utterances\")\n\nprint(\"\\n-------------------\")\nprint(\"OVERALL STATS FOR KODIS PREDICTIONS \\n-------------------\")\nmetrics_df = pd.DataFrame([metrics1_kwiki, metrics2_kcmv], index=['model_wiki','model_cmv'])\ndisplay(metrics_df)\n\n\nvals_kwiki = np.array(list(horizon_kwiki.values()))\nvals_kcmv  = np.array(list(horizon_kcmv.values()))\n\nmax_kwiki = vals_kwiki.max() if vals_kwiki.size else 1\nmax_kcmv  = vals_kcmv.max()  if vals_kcmv.size  else 1\n\nbins_kwiki = range(1, max_kwiki)\nbins_kcmv  = range(1, max_kcmv)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4), sharey=True)\n\nax1.hist(vals_kwiki, bins=bins_kwiki, density=True, edgecolor=\"k\")\nax1.set_title(\"Wiki model\\nForecast horizon\")\nax1.set_xlabel(\"Number of comments between first positive\\nforecast and end of conversation\")\nax1.set_ylabel(\"Percent of conversations\")\n\nax2.hist(vals_kcmv, bins=bins_kcmv, density=True, edgecolor=\"k\")\nax2.set_title(\"CMV model\\nForecast horizon\")\nax2.set_xlabel(\"Number of comments between first positive\\nforecast and end of conversation\")\ndisplay(\"wiki model horizon stats:  Mean = 9.723837209302326, Median = 9.0\")\ndisplay(\"cmv model horizon stats: Mean = 10.006060606060606, Median = 9.0\")\nplt.tight_layout()\nplt.show()\n\n\n# 3) Merge the two conversation‐level forecasts side by side\nconv1 = conv_kwiki.rename(columns={'label':'label','score':'score_wiki','forecast':'forecast_wiki'})\nconv2 = conv_kcmv.rename(columns={'score':'score_cmv','forecast':'forecast_cmv'})\n\n# join on conversation_id\nmerged = conv1.join(conv2[['score_cmv','forecast_cmv']], how='inner')\n# print(\"=== Conversation‑level forecasts comparison ===\")\n# display(merged)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.calibration import CalibrationDisplay\nfrom IPython.display import display\n\n\n# 1) Compute agreement flag\nmerged['match'] = merged['forecast_wiki'] == merged['forecast_cmv']\n\n# 2) Overall agreement rate\nagree_rate = merged['match'].mean()\nprint(f\"Agreement rate between wiki vs. cmv predictions: {agree_rate:.2%}\\n\")\n\n# 3) Where they disagree\nprint(\"Disagreement breakdown (wiki  vs. cmv ):\")\nprint(pd.crosstab(\n    merged.loc[~merged['match'], 'forecast_wiki'],\n    merged.loc[~merged['match'], 'forecast_cmv'],\n    rownames=['wiki_pred'], colnames=['cmv_pred']\n))\nprint(\"\\n\")\n\n# 4) Calibration + histogram side by side\nfig, (ax_cal, ax_hist) = plt.subplots(1, 2, figsize=(12, 4))\n\n# 4a) overlayed calibration curves on the left\nCalibrationDisplay.from_predictions(\n    y_true=merged['label'],\n    y_prob=merged['score_wiki'],\n    n_bins=10,\n    name=\"Wiki\",\n    ax=ax_cal\n)\nCalibrationDisplay.from_predictions(\n    y_true=merged['label'],\n    y_prob=merged['score_cmv'],\n    n_bins=10,\n    name=\"CMV\",\n    ax=ax_cal\n)\nax_cal.set_title(\"Calibration Curve (Reliability)\")\nax_cal.set_xlabel(\"Mean predicted probability\")\nax_cal.set_ylabel(\"Observed fraction\")\nax_cal.legend(loc=\"lower right\")\nax_cal.grid(True)\n\n# 4b) probability‐histogram on the right\nbins = np.linspace(0, 1, 11)\nax_hist.hist(merged['score_wiki'], bins=bins, alpha=0.6, label='Wiki')\nax_hist.hist(merged['score_cmv'],  bins=bins, alpha=0.6, label='CMV')\nax_hist.set_title(\"Probability Histogram\")\nax_hist.set_xlabel(\"Predicted probability\")\nax_hist.set_ylabel(\"Count of utterances\")\nax_hist.legend()\nax_hist.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n# 5) Confusion matrices side by side\nfig, (ax_wiki, ax_cmv) = plt.subplots(1, 2, figsize=(10, 4))\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=merged['label'],\n    y_pred=merged['forecast_wiki'],\n    display_labels=[\"Success\", \"Impasse\"],\n    cmap='Blues',\n    ax=ax_wiki\n)\nax_wiki.set_title(\"Wiki Model\")\n\nConfusionMatrixDisplay.from_predictions(\n    y_true=merged['label'],\n    y_pred=merged['forecast_cmv'],\n    display_labels=[\"Success\", \"Impasse\"],\n    cmap='Blues',\n    ax=ax_cmv\n)\nax_cmv.set_title(\"CMV Model\")\n\nplt.tight_layout()\nplt.show()\n\n# 6) Summary table\nsummary = pd.DataFrame({\n    'wiki_acc':       [(merged['label'] == merged['forecast_wiki']).mean()],\n    'cmv_acc':        [(merged['label'] == merged['forecast_cmv']).mean()],\n    'agreement_rate': [agree_rate],\n    'wiki_avg_prob':  [merged['score_wiki'].mean()],\n    'cmv_avg_prob':   [merged['score_cmv'].mean()],\n}, index=['utterance_level'])\nprint(\"\\nSummary statistics:\")\ndisplay(summary)\n\n\n\nAverage conversation length in KODIS is: 13.05 utterances\n\n-------------------\nOVERALL STATS FOR KODIS PREDICTIONS \n-------------------\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nmodel_wiki\n0.450403\n0.233220\n0.927224\n0.651498\n0.372698\n\n\nmodel_cmv\n0.240152\n0.174603\n0.889488\n0.898618\n0.291906\n\n\n\n\n\n\n\n'wiki model horizon stats:  Mean = 9.723837209302326, Median = 9.0'\n\n\n'cmv model horizon stats: Mean = 10.006060606060606, Median = 9.0'\n\n\n\n\n\n\n\n\n\nAgreement rate between wiki vs. cmv predictions: 69.96%\n\nDisagreement breakdown (wiki  vs. cmv ):\ncmv_pred   0.0  1.0\nwiki_pred          \n0.0          0  524\n1.0        109    0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\nwiki_acc\ncmv_acc\nagreement_rate\nwiki_avg_prob\ncmv_avg_prob\n\n\n\n\nutterance_level\n0.450403\n0.240152\n0.699573\n0.708614\n0.801742\n\n\n\n\n\n\n\n\n\nCode\nprint(merged['label'].value_counts())\nprint(merged['forecast_wiki'].value_counts())\nprint(merged['forecast_cmv'].value_counts())\n\n\nlabel\n0    1736\n1     371\nName: count, dtype: int64\nforecast_wiki\n1.0    1475\n0.0     632\nName: count, dtype: int64\nforecast_cmv\n1.0    1890\n0.0     217\nName: count, dtype: int64\n\n\n\n\nAverage length of tokenized utterances for wiki test , cmv test, and kodis corpora from CRAFT tokenization scheme\n\nCRAFT has max tokenization length of 80 tokens per utterance. Is this relevant to the performance in anyway? More specifically:\n\nwhat are the tokenization lengths of the utterances for the data used in the train sets for CMV and Wiki?\nwhat is the average tokenization length for a KODIS utterance?\n\n\nIs it even relevant to affecting performance, and how to measure this?\n\n\nCode\nfrom convokit import download, Corpus\n\n\ncorpus_cmv = Corpus(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets/conversations-gone-awry-cmv-corpus\")\ncorpus_wiki = Corpus(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets/conversations-gone-awry-corpus\")\n\n\n\n\nCode\nconvo_ex =  next(corpus_wiki.iter_conversations())\nprint(\"Conversation Meta data for wiki conversation \\n\"\n\"======================\")\nfor key, val in convo_ex.meta.items():\n  \n    print(f\"{key:20s} → {val}\")\n\n\nprint(\"\\n\")\nconvo_ex2 =  next(corpus_cmv.iter_conversations())\nprint(\"Conversation Meta data for cmv conversation \\n \"\n\"======================\")\nfor key, val in convo_ex2.meta.items():\n    print(f\"{key:20s} → {val}\")\n\n\nConversation Meta data for wiki conversation \n======================\npage_title           → User talk:2005\npage_id              → 1003212\npair_id              → 143890867.11926.11926\nconversation_has_personal_attack → False\nverified             → True\npair_verified        → True\nannotation_year      → 2018\nsplit                → train\n\n\nConversation Meta data for cmv conversation \n ======================\npair_id              → cue8uxd\nhas_removed_comment  → True\nsplit                → train\nsummary_meta         → []\n\n\n\n\nCode\nimport os\nimport numpy as np\nfrom convokit import download, Corpus\nfrom convokit.forecaster.CRAFT.data import (\n    loadPrecomputedVoc,\n    tokenize,\n    Voc,\n)\n\n'''\nNeed to get only the utterances used for training fine-tuned model. \nChoose only those utterances in conversations where mete.split == train\n'''\n\ndef load_craft_vocab(model_name: str) -&gt; Voc:\n    base = download(model_name, data_dir=os.path.expanduser(\"~/.convokit/models\"))\n    return loadPrecomputedVoc(\n        model_name,\n        os.path.join(base, \"word2index.json\"),\n        os.path.join(base, \"index2word.json\"),\n    )\n\n\nvoc_wiki = load_craft_vocab(\"craft-wiki-finetuned\")\nvoc_cmv  = load_craft_vocab(\"craft-cmv-finetuned\")\n\n\n\n\nCode\n\"\"\"\n    Walk through all conversations in `corpus`, select only those with\n    convo.meta[\"split\"] == split, tokenize each utterance via CRAFT's\n    tokenize(voc, text), and return summary stats on token counts.\n\"\"\"\n\ndef summarize_token_lengths(corpus: Corpus, voc: Voc, split: str = None):\n    lengths = []\n    for convo in corpus.iter_conversations():\n        # if a split is specified, filter; otherwise include everything\n        if split is not None and convo.meta.get(\"split\") != split:\n            continue\n        for utt in convo.iter_utterances():\n            toks = tokenize(voc, utt.text or \"\")\n            lengths.append(len(toks))\n    if not lengths:\n        return {\"n_utt\": 0, \"mean\": np.nan, \"median\": np.nan, \"std\": np.nan}\n    arr = np.array(lengths)\n    return {\n        \"n_utt\":  int(arr.size),\n        \"mean\":   float(arr.mean()),\n        \"median\": float(np.median(arr)),\n        \"std\":    float(arr.std()),\n    }\n\n\n\n# -- 5) run it on all four settings --\nresults = {\n    \"craft‑wiki\":   summarize_token_lengths(corpus_wiki, voc_wiki, split=\"train\"),\n    \"craft‑cmv\":    summarize_token_lengths(corpus_cmv,  voc_cmv,  split=\"train\"),\n    \"kodis‑wiki\":   summarize_token_lengths(corp1, voc_wiki),\n    \"kodis‑cmv\":    summarize_token_lengths(corp1, voc_cmv),\n\n}\n\ndf = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"vocab\"})\ndisplay(df)\n\n\n\n\n\n\n\n\n\nvocab\nn_utt\nmean\nmedian\nstd\n\n\n\n\n0\ncraft‑wiki\n18042.0\n82.001330\n47.0\n139.549799\n\n\n1\ncraft‑cmv\n25885.0\n123.472474\n72.0\n166.329963\n\n\n2\nkodis‑wiki\n27498.0\n24.159939\n20.0\n20.964779\n\n\n3\nkodis‑cmv\n27498.0\n24.159939\n20.0\n20.964779\n\n\n\n\n\n\n\n\n\n\nCompare Predictions on test sets for CMV and WIKI datasets using respective fine-tuned CRAFT Models\n\nfor reproducability, need to check if any additions since 2018 for wiki data as it is public and growing. reddit cmv was created specifically for this paper, so no updates expected.\n\n\n\nCode\ndef transform_selector(context_tuple):\n    \"\"\"\n    For transform we only need to check that the conversation is in the test split\n    \"\"\"\n    return (context_tuple.current_utterance.get_conversation().meta[\"split\"] == \"test\")\n\ncorpus_cmv = Corpus(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets/conversations-gone-awry-cmv-corpus\")\ncorpus_wiki = Corpus(\"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets/conversations-gone-awry-corpus\")\n\n\n\n\nCode\ncorp_test_wiki = forecaster_wiki.transform(corpus_wiki, context_selector= transform_selector)\ncorp_test_cmv = forecaster_cmv.transform(corpus_cmv, context_selector= transform_selector)\n\n\n\n\nCode\n%%capture\nimport numpy as np\nimport matplotlib.pyplot as plt\ntest_selector = lambda c: c.meta.get(\"split\") == \"test\"\nhorizon_wiki = forecaster_wiki._draw_horizon_plot(corp_test_wiki, test_selector)\nhorizon_cmv  = forecaster_cmv._draw_horizon_plot(corp_test_cmv, test_selector)\nconv_df1, metrics1 = forecaster_wiki.summarize(corp_test_wiki,  lambda c: c.meta['split'] == \"test\")\nconv_df2, metrics2 = forecaster_cmv.summarize(corp_test_cmv,  lambda c: c.meta['split'] == \"test\")\n\n\n\n\nCode\n\nlengths_cmv = [\n    len(convo.get_utterance_ids())\n    for convo in corp_test_cmv.iter_conversations()\n]\nlengths_wiki = [\n    len(convo.get_utterance_ids())\n    for convo in corp_test_wiki.iter_conversations()\n]\navg_len_cmv = sum(lengths_cmv) / len(lengths_cmv)\navg_len_wiki = sum(lengths_wiki) / len(lengths_wiki)\nprint(f\"Average conversation length in test WIKI is: {avg_len_wiki:.2f} utterances\")\nprint(f\"Average conversation length in test CMV is: {avg_len_cmv:.2f} utterances\")\nprint(\"\\n-------------------\")\nprint(\"OVERALL STATS FOR TEST SET PREDICTIONS \\n -------------------\")\nmetrics_df = pd.DataFrame([metrics1, metrics2], index=['corp_wiki_test','copr_cmv_test'])\ndisplay(metrics_df)\n\n\nvals_wiki = np.array(list(horizon_wiki.values()))\nvals_cmv  = np.array(list(horizon_cmv.values()))\n\nmax_wiki = vals_wiki.max() if vals_wiki.size else 1\nmax_cmv  = vals_cmv.max()  if vals_cmv.size  else 1\n\nbins_wiki = range(1, max_wiki)\nbins_cmv  = range(1, max_cmv)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4), sharey=True)\n\nax1.hist(vals_wiki, bins=bins_wiki, density=True, edgecolor=\"k\")\nax1.set_title(\"Wiki model\\nForecast horizon\")\nax1.set_xlabel(\"Number of comments between first positive\\nforecast and end of conversation\")\nax1.set_ylabel(\"Percent of conversations\")\n\nax2.hist(vals_cmv, bins=bins_cmv, density=True, edgecolor=\"k\")\nax2.set_title(\"CMV model\\nForecast horizon\")\nax2.set_xlabel(\"Number of comments between first positive\\nforecast and end of conversation\")\n\nplt.tight_layout()\nplt.show()\n\n\nAverage conversation length in test WIKI is: 7.17 utterances\nAverage conversation length in test CMV is: 6.28 utterances\n\n-------------------\nOVERALL STATS FOR TEST SET PREDICTIONS \n -------------------\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\ncorp_wiki_test\n0.704762\n0.638264\n0.945238\n0.535714\n0.761996\n\n\ncopr_cmv_test\n0.620614\n0.589967\n0.790936\n0.549708\n0.675828"
  },
  {
    "objectID": "src/fine-tuning_final.html",
    "href": "src/fine-tuning_final.html",
    "title": "Fine-Tining Kodis",
    "section": "",
    "text": "I spent a lot of time looking at CRAFT under the hood this week to see possible modifications. This initial “exploratory” fine-tuning notebook ended up necessitating using cloud compute(used deepnote) for fine-tuning (GP4 -L4 $1.56/hr) to get some initial results, as each run for fine-tuning one model with training size (1264) was taking 40 min to train on my machine. To really improve results, I would look into setting up full end-to-end with python scripts to manage experiment runs better. I did a couple this week (1-3 for each model variant) in this notebook, and there is still randomness that is not reproducible across experiment runs. There is a lot fo room to modify the “model_config” for CRAFT and perhaps do dynamic adjustments to the learning rate for each batch, as I had issues with overfitting, but this notebook is not generalizble to test at this level.\nWhat does “doing better” mean to know if we should continue? Pivot to analysis with self-report scores?\n\nIt is pretty ineffecient to continue workin in jupyter notebook for fine-tuning as this is no way a robust training pipeline, but I wanted to touch base to see if the above would make sense to continue trying\n\nThe few downsampling training runs had high accuracy variablitiy (27% to 80%)\nThere are some clear upward patterns in the prediciton scores broken out by success/impasse’s emerging\ndownsampling does affect the accuracy quite a bit\nAUC/PR curves look promising. I also chose the best threshold and plotted metric comparisons as well as the Confusion Matrix across each variant."
  },
  {
    "objectID": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighteddownsampled-variants",
    "href": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighteddownsampled-variants",
    "title": "Fine-Tining Kodis",
    "section": "Ground Model Performance Comparison Across Default, Weighted,Downsampled Variants",
    "text": "Ground Model Performance Comparison Across Default, Weighted,Downsampled Variants\n\n\nCode\ncorpora_info_ground = [\n    (\n        \"GROUND_DEFAULT\",\n        no_samp   [\"ground_corpus\"],\n        no_samp   [\"ground_metrics\"],\n        no_samp   [\"ground_df\"],\n        no_samp   [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_WEIGHTED\",\n        wt        [\"ground_corpus\"],\n        wt        [\"ground_metrics\"],\n        wt        [\"ground_df\"],\n        wt        [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_DOWNSAMPLED\",\n        down      [\"ground_corpus\"],\n        down      [\"ground_metrics\"],\n        down      [\"ground_df\"],\n        down      [\"ground_horizon\"],\n    ),\n    (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nbest_thresholds_ground, best_metrics_ground, best_corpora_ground = compare_craft_models(corpora_info_ground)\n\n\n== Avg. Conversation Length ==\n  GROUND_DEFAULT        train=13.0  test=13.1\n  GROUND_WEIGHTED       train=13.1  test=13.1\n  GROUND_DOWNSAMPLED    train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nGROUND_DEFAULT\n0.303318\n0.198910\n1.000000\n0.842407\n0.331818\n\n\nGROUND_WEIGHTED\n0.175355\n0.173397\n1.000000\n0.997135\n0.295547\n\n\nGROUND_DOWNSAMPLED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGROUND_DEFAULT       best thr=0.541, TPR=1.000, FPR=0.000, J=1.000\nGROUND_WEIGHTED      best thr=0.575, TPR=0.699, FPR=0.072, J=0.627\nGROUND_DOWNSAMPLED   best thr=0.581, TPR=0.671, FPR=0.112, J=0.559\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nGROUND_DEFAULT_acc\nGROUND_WEIGHTED_acc\nGROUND_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nGROUND_DEFAULT_avg_prob\nGROUND_WEIGHTED_avg_prob\nGROUND_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.303318\n0.175355\n0.172986\n0.443128\n0.519425\n0.560313\n0.56887\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Performance Comparison\n\n\nCode\ncompare_best_models(best_thresholds_ground, best_metrics_ground, best_corpora_ground)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nGROUND_DEFAULT\n1.000000\n1.000000\n1.000000\n0.000000\n1.000000\n0.541032\n\n\nGROUND_WEIGHTED\n0.827014\n0.000000\n0.000000\n0.000000\n0.000000\n0.575249\n\n\nGROUND_DOWNSAMPLED\n0.850711\n0.556818\n0.671233\n0.111748\n0.608696\n0.580783\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/root/venv/lib/python3.11/site-packages/numpy/lib/_histograms_impl.py:902: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-last-utterance-model-performance-comparison-across-default-weighteddownsampled-variants",
    "href": "src/fine-tuning_final.html#no-last-utterance-model-performance-comparison-across-default-weighteddownsampled-variants",
    "title": "Fine-Tining Kodis",
    "section": "No Last Utterance Model Performance Comparison Across Default, Weighted,Downsampled Variants",
    "text": "No Last Utterance Model Performance Comparison Across Default, Weighted,Downsampled Variants\n\n\nCode\ncorpora_info_no_last = [\n    (\n        \"NO_LAST_DEFAULT\",\n        no_samp[\"no_last_corpus\"],\n        no_samp[\"no_last_metrics\"],\n        no_samp[\"no_last_df\"],\n        no_samp[\"no_last_horizon\"],\n    ),\n    (\n        \"NO_LAST_WEIGHTED\",\n        wt[\"no_last_corpus\"],\n        wt[\"no_last_metrics\"],\n        wt[\"no_last_df\"],\n        wt[\"no_last_horizon\"],\n    ),\n    (\n        \"NO_LAST_DOWNSAMPLED\",\n        down[\"no_last_corpus\"],\n        down[\"no_last_metrics\"],\n        down[\"no_last_df\"],\n        down[\"no_last_horizon\"],\n    ),\n    (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nbest_thresholds_no_last, best_metrics_no_last, best_corpora_no_last = compare_craft_models(corpora_info_no_last)\n\n\n== Avg. Conversation Length ==\n  NO_LAST_DEFAULT       train=13.0  test=13.1\n  NO_LAST_WEIGHTED      train=13.1  test=13.1\n  NO_LAST_DOWNSAMPLED   train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nNO_LAST_DEFAULT\n0.175355\n0.173397\n1.000000\n0.997135\n0.295547\n\n\nNO_LAST_WEIGHTED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nNO_LAST_DOWNSAMPLED\n0.172986\n0.172986\n1.000000\n1.000000\n0.294949\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNO_LAST_DEFAULT      best thr=0.555, TPR=0.699, FPR=0.499, J=0.200\nNO_LAST_WEIGHTED     best thr=0.577, TPR=0.630, FPR=0.441, J=0.189\nNO_LAST_DOWNSAMPLED  best thr=0.560, TPR=0.890, FPR=0.665, J=0.226\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nNO_LAST_DEFAULT_acc\nNO_LAST_WEIGHTED_acc\nNO_LAST_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nNO_LAST_DEFAULT_avg_prob\nNO_LAST_WEIGHTED_avg_prob\nNO_LAST_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.175355\n0.172986\n0.172986\n0.443128\n0.557905\n0.574563\n0.568332\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Performance Comparison\n\n\nCode\ncompare_best_models(best_thresholds_no_last, best_metrics_no_last, best_corpora_no_last)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nNO_LAST_DEFAULT\n0.535545\n0.226667\n0.698630\n0.498567\n0.342282\n0.555385\n\n\nNO_LAST_WEIGHTED\n0.827014\n0.000000\n0.000000\n0.000000\n0.000000\n0.576696\n\n\nNO_LAST_DOWNSAMPLED\n0.431280\n0.218855\n0.890411\n0.664756\n0.351351\n0.559719\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/root/venv/lib/python3.11/site-packages/numpy/lib/_histograms_impl.py:902: RuntimeWarning: invalid value encountered in divide\n  return n/db/n.sum(), bin_edges"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tining Kodis",
    "section": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\n\ncorpora_info_no_subm = [\n    (\n        \"NO_LAST_SUBMIT_DEFAULT\",\n        no_samp[\"no_subm_corpus\"],\n        no_samp[\"no_subm_metrics\"],\n        no_samp[\"no_subm_df\"],\n        no_samp[\"no_subm_horizon\"],\n    ),\n    (\n        \"NO_LAST_SUBMIT_WEIGHTED\",\n        wt[\"no_subm_corpus\"],\n        wt[\"no_subm_metrics\"],\n        wt[\"no_subm_df\"],\n        wt[\"no_subm_horizon\"],\n    ),\n    (\n        \"NO_LAST_SUBMIT_DOWNSAMPLED\",\n        down[\"no_subm_corpus\"],\n        down[\"no_subm_metrics\"],\n        down[\"no_subm_df\"],\n        down[\"no_subm_horizon\"],\n    ), (\n        \"GROUND_WIKI\",\n        wiki      [\"ground_corpus\"],\n        wiki      [\"ground_metrics\"],\n        wiki      [\"ground_df\"],\n        wiki      [\"ground_horizon\"],\n    )\n]\n\nno_last_submit_thresholds, no_last_submit_metrics, no_last_submit_corpora = compare_craft_models(corpora_info_no_subm)\n\n\n== Avg. Conversation Length ==\n  NO_LAST_SUBMIT_DEFAULT  train=13.0  test=13.1\n  NO_LAST_SUBMIT_WEIGHTED  train=13.1  test=13.1\n  NO_LAST_SUBMIT_DOWNSAMPLED  train=13.0  test=13.1\n  GROUND_WIKI           train=13.0  test=13.1\n\n== Conversation‑level Test Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT\n0.670616\n0.318681\n0.794521\n0.355301\n0.454902\n\n\nNO_LAST_SUBMIT_WEIGHTED\n0.777251\n0.370370\n0.410959\n0.146132\n0.389610\n\n\nNO_LAST_SUBMIT_DOWNSAMPLED\n0.255924\n0.188630\n1.000000\n0.899713\n0.317391\n\n\nGROUND_WIKI\n0.443128\n0.228188\n0.931507\n0.659026\n0.366577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT best thr=0.521, TPR=0.781, FPR=0.321, J=0.460\nNO_LAST_SUBMIT_WEIGHTED best thr=0.497, TPR=0.699, FPR=0.252, J=0.446\nNO_LAST_SUBMIT_DOWNSAMPLED best thr=0.758, TPR=0.753, FPR=0.232, J=0.521\nGROUND_WIKI          best thr=0.872, TPR=0.740, FPR=0.264, J=0.476\n\n\n\n\n\n\n\n\n\n== Summary of Convo Acc & Avg Prob ==\n\n\n\n\n\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT_acc\nNO_LAST_SUBMIT_WEIGHTED_acc\nNO_LAST_SUBMIT_DOWNSAMPLED_acc\nGROUND_WIKI_acc\nNO_LAST_SUBMIT_DEFAULT_avg_prob\nNO_LAST_SUBMIT_WEIGHTED_avg_prob\nNO_LAST_SUBMIT_DOWNSAMPLED_avg_prob\nGROUND_WIKI_avg_prob\n\n\n\n\nconversation_level\n0.670616\n0.777251\n0.255924\n0.443128\n0.508492\n0.495041\n0.677392\n0.712573\n\n\n\n\n\n\n\n\nBest Threshold Model Performance Comparison\n\n\nCode\ncompare_best_models(no_last_submit_thresholds, no_last_submit_metrics, no_last_submit_corpora)\n\n\n== Conversation‑level Best Threshold Test Set Metrics ==\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nFPR\nF1\nThreshold\n\n\n\n\nNO_LAST_SUBMIT_DEFAULT\n0.696682\n0.337278\n0.780822\n0.320917\n0.471074\n0.521406\n\n\nNO_LAST_SUBMIT_WEIGHTED\n0.739336\n0.366906\n0.698630\n0.252149\n0.481132\n0.497068\n\n\nNO_LAST_SUBMIT_DOWNSAMPLED\n0.765403\n0.404412\n0.753425\n0.232092\n0.526316\n0.757998\n\n\nGROUND_WIKI\n0.736967\n0.369863\n0.739726\n0.263610\n0.493151\n0.871697"
  },
  {
    "objectID": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#ground-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tining Kodis",
    "section": "Ground Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "Ground Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"ground_corpus\"], name = \"Ground Default\")\nplot_position_score_evolution_by_outcome(down[\"ground_corpus\"], name = \"Ground Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"ground_corpus\"], name = \"Ground Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-last-utt-model-performance-comparison-across-default-weighted-downsampled-variants",
    "href": "src/fine-tuning_final.html#no-last-utt-model-performance-comparison-across-default-weighted-downsampled-variants",
    "title": "Fine-Tining Kodis",
    "section": "No Last Utt Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last Utt Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"no_last_corpus\"], name = \":No Last Default\")\nplot_position_score_evolution_by_outcome(down[\"no_last_corpus\"], name = \":No Last Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"no_last_corpus\"], name = \":No Last Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  },
  {
    "objectID": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants-1",
    "href": "src/fine-tuning_final.html#no-lastno-submit-model-performance-comparison-across-default-weighted-downsampled-variants-1",
    "title": "Fine-Tining Kodis",
    "section": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants",
    "text": "No Last/No Submit Model Performance Comparison Across Default, Weighted, Downsampled Variants\n\n\nCode\nimport numpy as np\nplot_position_score_evolution_by_outcome(no_samp[\"no_subm_corpus\"], name = \":No Last/No Submit Default\")\nplot_position_score_evolution_by_outcome(down[\"no_subm_corpus\"], name = \":No Last/No Submit Downsampled\")\nplot_position_score_evolution_by_outcome(wt[\"no_subm_corpus\"], name = \":No Last/No Submit Weighted\")\nplot_position_score_evolution_by_outcome(wiki[\"ground_corpus\"], name = \"Ground Wiki\")"
  }
]