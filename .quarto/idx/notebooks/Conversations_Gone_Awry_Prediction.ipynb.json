{"title":"Predicting Conversations Gone Awry With Convokit","markdown":{"yaml":{"title":"Predicting Conversations Gone Awry With Convokit"},"headingText":"Step 1: Load the corpus","containsRefs":false,"markdown":"\n\n\n\n\nThis interactive tutorial demonstrates how to predict whether a conversation will eventually lead to a personal attack, as seen in the paper [Conversations Gone Awry: Detecting Early Signs of Conversational Failure](http://www.cs.cornell.edu/~cristian/Conversations_gone_awry.html), using the tools provided by ConvoKit. It also serves as an illustration of how to use two transformers implemented by ConvoKit: PromptTypes and politeness strategies.\n\nNote that due to updates in the implementation of ConvoKit modules as well as libraries we've used, the results in the notebook deviate slightly from what was presented in the paper; however, the overall findings match fairly closely.\nNOTE: for an updated version of the `PromptTypes` method, using the Expected Conversational Context [Framework](https://tisjune.github.io/research/dissertation) and the `ExpectedContextModel` [functionality](https://convokit.cornell.edu/documentation/expected_context_model.html), see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/convokit/expected_context_framework/demos/wiki_awry_demo.ipynb), which uses the same datasets. We keep this demo around to more closely match with the original publication.\n\n\n\n\nThe dataset from the Conversations Gone Awry paper is provided through Convokit as \"conversations-gone-awry-corpus\". We will download this corpus, which includes precomputed SpaCy dependency parses, and use it for our analysis.\n\nThe original Awry paper and dataset was released at ACL 2018. In the time since initial release, the dataset has been expanded with additional labeled samples, with the later annotation rounds focusing on longer conversations. Since this example notebook is based on the original Awry paper, we will now filter the corpus to keep only the conversations from the original dataset. This can be achieved by checking the \"annotation_year\" metadata entry; original data will have the value \"2018\". However, if you may also skip this filtering stage (skip directly to \"Step 2: Extract prompt types features\") if you wish to run the model on the complete dataset (note that the notebook will take quite a while longer to run).\n\n## Step 2: Extract prompt types features\n\nIn this step, we will extract the first of the two types of pragmatic features seen in the paper: prompt types. We can learn prompt types and compute types for each utterance in the corpus using ConvoKit's PromptTypeWrapper class (which implements an end-to-end pipeline that extracts prompt types; see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/examples/prompt-types/prompt-type-demo.ipynb) for examples of particular steps in that pipeline). Note that in keeping with proper machine learning practices, we need a different dataset to use for training the PromptTypeWrapper object. For this, we will use Convokit's Wikipedia talk corpus (\"wiki-corpus\"), which is composed of Wikipedia talk page conversations different from those found in the Awry corpus.\n\nWe will train a PromptTypeWrapper object on the downloaded corpus. \n\n(demo continues after long output)\n\nHere are the six prompt types that our model has inferred -- i.e., prototypical features of the prompts and responses, as well as prototypical utterances for each type. \n\nNote that due to updates in SpaCy's dependency parsing, as well as in our implementation of the prompt types methodology, the particular types returned don't fully align with what's in the paper (though we see that they recover similar prompt types, and down the line may result in better performance accuracies). \n\ndemo continues after long output\n\nWe've given them the following names.\n\nNow that the prompt types model has been trained, we can use it to compute prompt types for our present Awry corpus (note that this is a _different_ corpus than what the original model was trained on)\n\nNow we want to gather the computed prompt types into a tabular format to use as features for an sklearn estimator. For our purposes, we want the distances from the centers of the KMeans clusters corresponding to each prompt type.\n\nLater in our analysis, we will also use the particular prompt type that has been assigned to each utterance:\n\n## Step 3: Extract politeness strategies features\n\nNow we will extract the second type of pragmatic features described in the paper: politeness strategies. We can do this using convokit's PolitenessStrategies class. This class does not require any training, so we can just apply it directly to the corpus.\n\nSimilar to before, we will construct a feature matrix from the computed per-utterance features. In this case, the results can be found in the `politeness_strategies` metadata field.\n\n## Step 4: Create pair data\n\nThe analyses performed in the paper, and the prediction task, considers _pairs_ of conversations. The corpus downloaded from convokit already includes metadata about how conversations were paired for the paper, so we don't need to do any of the hard work here. Instead, we'll format the pair information into a table for use in prediction.\n\n# Step 5: Comparing features exhibited\n\nWe can compare how often the pragmatic devices we extracted occur in the initial exchanges of conversations that turn awry, vs. conversations that stay on track. We will compute log-odds ratios of each device, comparing the awry and on-track conversations; we will also compute significance values from binomal tests comparing the proportion of awry-turning conversations exhibiting a particular device to the proportion of on-track conversations.\n\nSince we've already got the pragmatic features precomputed, and our dataset of pairs compiled, it remains to compute the effect sizes and statistical significances, and plot these values, producing a plot like Figure 2 from the paper.\n\nNote: due to changes in SpaCy's dependency parsing that took place between the original time of publication and the updated release of this code, the extracted features may differ slightly from the ones used in the paper, so the resulting figure may differ slightly from the one in the paper.\n\n## Step 6: Construct feature matrix\n\nTo run the prediction task, we can construct a table of pragmatic features for each pair, to use in prediction. This table will consist of the prompt types and politeness strategies for the first and second comment of each conversation.\n\n## Step 7: Prediction Utils\n\nWe're almost ready to do the prediction! First we need to define a few helper functions...\n\n## Step 8: Prediction\n\nFinally, we run the prediction task on each possible combination of pragmatic features: prompt types, politeness strategies, and both combined. We generate a table like Table 3 from the paper.\n","srcMarkdownNoYaml":"\n\n\n\n\nThis interactive tutorial demonstrates how to predict whether a conversation will eventually lead to a personal attack, as seen in the paper [Conversations Gone Awry: Detecting Early Signs of Conversational Failure](http://www.cs.cornell.edu/~cristian/Conversations_gone_awry.html), using the tools provided by ConvoKit. It also serves as an illustration of how to use two transformers implemented by ConvoKit: PromptTypes and politeness strategies.\n\nNote that due to updates in the implementation of ConvoKit modules as well as libraries we've used, the results in the notebook deviate slightly from what was presented in the paper; however, the overall findings match fairly closely.\nNOTE: for an updated version of the `PromptTypes` method, using the Expected Conversational Context [Framework](https://tisjune.github.io/research/dissertation) and the `ExpectedContextModel` [functionality](https://convokit.cornell.edu/documentation/expected_context_model.html), see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/convokit/expected_context_framework/demos/wiki_awry_demo.ipynb), which uses the same datasets. We keep this demo around to more closely match with the original publication.\n\n\n\n## Step 1: Load the corpus\n\nThe dataset from the Conversations Gone Awry paper is provided through Convokit as \"conversations-gone-awry-corpus\". We will download this corpus, which includes precomputed SpaCy dependency parses, and use it for our analysis.\n\nThe original Awry paper and dataset was released at ACL 2018. In the time since initial release, the dataset has been expanded with additional labeled samples, with the later annotation rounds focusing on longer conversations. Since this example notebook is based on the original Awry paper, we will now filter the corpus to keep only the conversations from the original dataset. This can be achieved by checking the \"annotation_year\" metadata entry; original data will have the value \"2018\". However, if you may also skip this filtering stage (skip directly to \"Step 2: Extract prompt types features\") if you wish to run the model on the complete dataset (note that the notebook will take quite a while longer to run).\n\n## Step 2: Extract prompt types features\n\nIn this step, we will extract the first of the two types of pragmatic features seen in the paper: prompt types. We can learn prompt types and compute types for each utterance in the corpus using ConvoKit's PromptTypeWrapper class (which implements an end-to-end pipeline that extracts prompt types; see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/examples/prompt-types/prompt-type-demo.ipynb) for examples of particular steps in that pipeline). Note that in keeping with proper machine learning practices, we need a different dataset to use for training the PromptTypeWrapper object. For this, we will use Convokit's Wikipedia talk corpus (\"wiki-corpus\"), which is composed of Wikipedia talk page conversations different from those found in the Awry corpus.\n\nWe will train a PromptTypeWrapper object on the downloaded corpus. \n\n(demo continues after long output)\n\nHere are the six prompt types that our model has inferred -- i.e., prototypical features of the prompts and responses, as well as prototypical utterances for each type. \n\nNote that due to updates in SpaCy's dependency parsing, as well as in our implementation of the prompt types methodology, the particular types returned don't fully align with what's in the paper (though we see that they recover similar prompt types, and down the line may result in better performance accuracies). \n\ndemo continues after long output\n\nWe've given them the following names.\n\nNow that the prompt types model has been trained, we can use it to compute prompt types for our present Awry corpus (note that this is a _different_ corpus than what the original model was trained on)\n\nNow we want to gather the computed prompt types into a tabular format to use as features for an sklearn estimator. For our purposes, we want the distances from the centers of the KMeans clusters corresponding to each prompt type.\n\nLater in our analysis, we will also use the particular prompt type that has been assigned to each utterance:\n\n## Step 3: Extract politeness strategies features\n\nNow we will extract the second type of pragmatic features described in the paper: politeness strategies. We can do this using convokit's PolitenessStrategies class. This class does not require any training, so we can just apply it directly to the corpus.\n\nSimilar to before, we will construct a feature matrix from the computed per-utterance features. In this case, the results can be found in the `politeness_strategies` metadata field.\n\n## Step 4: Create pair data\n\nThe analyses performed in the paper, and the prediction task, considers _pairs_ of conversations. The corpus downloaded from convokit already includes metadata about how conversations were paired for the paper, so we don't need to do any of the hard work here. Instead, we'll format the pair information into a table for use in prediction.\n\n# Step 5: Comparing features exhibited\n\nWe can compare how often the pragmatic devices we extracted occur in the initial exchanges of conversations that turn awry, vs. conversations that stay on track. We will compute log-odds ratios of each device, comparing the awry and on-track conversations; we will also compute significance values from binomal tests comparing the proportion of awry-turning conversations exhibiting a particular device to the proportion of on-track conversations.\n\nSince we've already got the pragmatic features precomputed, and our dataset of pairs compiled, it remains to compute the effect sizes and statistical significances, and plot these values, producing a plot like Figure 2 from the paper.\n\nNote: due to changes in SpaCy's dependency parsing that took place between the original time of publication and the updated release of this code, the extracted features may differ slightly from the ones used in the paper, so the resulting figure may differ slightly from the one in the paper.\n\n## Step 6: Construct feature matrix\n\nTo run the prediction task, we can construct a table of pragmatic features for each pair, to use in prediction. This table will consist of the prompt types and politeness strategies for the first and second comment of each conversation.\n\n## Step 7: Prediction Utils\n\nWe're almost ready to do the prediction! First we need to define a few helper functions...\n\n## Step 8: Prediction\n\nFinally, we run the prediction task on each possible combination of pragmatic features: prompt types, politeness strategies, and both combined. We generate a table like Table 3 from the paper.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Conversations_Gone_Awry_Prediction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","title":"Predicting Conversations Gone Awry With Convokit"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}