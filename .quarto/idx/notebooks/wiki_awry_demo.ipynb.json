{"title":"Applying the Expected Context Framework to Wikipedia discussions","markdown":{"yaml":{"title":"Applying the Expected Context Framework to Wikipedia discussions"},"headingText":"1. Loading and preprocessing the training data","containsRefs":false,"markdown":"\n\n\n\n\nThis notebook demonstrates an application of the Expected Context Framework to analyze Wikipedia talk page discussions. We are working up to the following question: what types of comments are more likely to start conversations that eventually derail into toxic personal attacks, versus conversations that remain civil throughout? The notebook goes over a few key function calls along the way. See [this paper](https://www.cs.cornell.edu/~cristian/Conversations_gone_awry_files/conversations_gone_awry.pdf) for details on the analysis question and datasets, and [this dissertation](https://tisjune.github.io/research/dissertation) for details on the framework and more comments on the below analyses.\n\n\n\nFor this demo, we start by training an Expected Context Model on a collection of Wikipedia discussions (different from the dataset we later analyze). In short, we want to infer different _rhetorical types_ of comments that occur at the start of these discussions. Via the framework, we'll derive representations of comments based on their expected replies (the \"forwards context\" of the conversation), and then infer types of comments by clustering these representations. We interpret the resultant clusters as different rhetorical types.\n\nFor an extended version of this analysis, on a slightly older implementation of the model (using the `PromptTypes` functionality), see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb).\n\n\nWe represent Wikipedia comments as dependency-parse arcs; in order to capture  rhetorical rather than topical information, we've removed nouns. These features are included with the release of the corpus, and can be loaded as follows:\n\nTo make sure that the arcs are stored in the right format (i.e., as strings, not lists), we apply the following transformer:\n\nIn applying the framework, we need to associate utterances with their replies. One comment can receive multiple replies; here, we arbitrarily choose one (noting that future work could make better use of this additional information). \n\n## 2. Applying the Expected Context Framework\n\n\nTo apply the Expected Context Framework, we start by converting the input utterance text to an input vector representation. Here, we represent utterances in a term-document matrix that's _normalized by columns_ (empirically, we found that this ensures that the representations derived by the framework aren't skewed by the relative frequency of utterances). We use the `ColNormedTfidfTransformer` transformer to do this.\n\nWe found that it was preferable to derive different tf-idf representations (and hence different inverse document frequencies, normalization terms, and vocabularies) for comments that receive replies, and utterances that have predecessors. We make this partitioning by passing in different selectors to the `fit` functions below.\n\nThe two sets are of course overlapping, but the former set mostly contains comments that start conversations, and the second contains more comments that reply to conversation-starters. We note that we'd expect the language of conversation-starters and repliers to be slightly different (i.e., the former tend to be requests, and the latter tend to be responses to these requests).\n\nWe then initialize an Expected Context Model:\n* we specify that the conversational context we will derive our comment representations from is the reply, `context_field=next_id`\n* as input, the model will use tf-idf representations from `vect_field=first_tfidf` to represent comments, and `context_vect_field=second_tfidf` to represent context comments\n* we'll derive `n_svd_dims=25`-dimensional representations, and infer `n_clusters=6` clusters\n* to infer these clusters, we will cluster _term_-level representations, `cluster_on='terms'`. Note that by default, the model will cluster _comment_-level representations, but in this context (perhaps since comments can get fairly long and unstructured), clustering on terms produces more interpretable output.\n\n(note that the following call takes some time to run)\n\n### Inferred comment types\n\nBelow, we print representative terms, comments, and context terms and comments for the clusters we've inferred (note that the output is quite long)\n\ndemo continues below\n\nPer our interpretation, we assign the following names to these clusters:\n\n## 2. Application to analysis data\n\nWe now use the comment types we've just derived to analyze a dataset containing conversations that eventually derail into toxic behavior, and conversations that stay on track throughout. We load this data:\n\nWe start by extracting the same noun-less dependency parse arcs that we used when training the model, above. To do this, we need to load the dependency parses, and then apply a few additional transformers, below:\n\nWe represent the comments from the new dataset as tf-idf vectors, using the vocabulary and parameters we've derived over the training data:\n\nFinally, we apply the trained model to transform the new dataset. In particular, we annotate each comment with an attribute, `fw_clustering.cluster`, that denotes the comment type it's assigned to:\n\nTo facilitate subsequent analyses, we will gather the comment types for each comment into a table:\n\n## 3. Analysis: comparing comment types in awry versus on-track conversations\n\nWe start by preprocessing the data, to facilitate our comparison of awry versus on-track conversations. Ultimately, we will compare the occurrence of comment types in the first and second comments of these discussions.\n\nWe compute log-odds ratios of each comment type, comparing the awry and on-track conversations. We will also compute significance values from binomal tests comparing the proportion of awry-turning conversations exhibiting a particular comment type to the proportion of on-track conversations. \n\nPlotting the resultant log-odds ratios, we note some differences in which types of comments tend to start awry versus on track discussions: _contentious_ comments tend to signal future troubles, while _coordinating_ work is a signal that the conversation will remain civil throughout.\n\n## 4. Pipeline usage\n\nWe can also apply the framework via a pipeline that handles the following:\n* processes text (via a pipeline supplied by the user; see cell below)\n* transforms text to input representation (via `ColNormedTfidfTransformer`)\n* derives framework output (via `ExpectedContextModelTransformer`)\n\nWe initialize the pipeline with the following arguments:\n* `text_field` specifies which utterance metadata field to use as text input.\n* `share_tfidf_models` indicates whether we want to train one `ColNormedTfidfTransformer` model for both utterances and context-utterances. In this case, since we want different input representations for first and second comments, we set this argument to `False`.\n* `text_pipe` specifies the pipeline used to compute the contents of `text_field`\n* `tfidf_params` specifies the parameters to be passed into the underlying `ColNormedTfidfTransformer` object\n* `min_terms` specifies the minimum number of terms in the vocabulary that an utterance must contain for it to be considered in fitting and transforming the underlying `ExpectedContextModelTransformer` object (see the `selector` argument passed into `ec_fw.fit` above)\n\nAll other arguments are inherited from `ExpectedContextModelTransformer`.\n\nThis should produce the same output as calling the constituent steps separately.\n\nThe pipeline enables us to transform ad-hoc string input.\n\nHere, instead of storing vector representations with a corpus, the pipeline writes these representations to a field in the utterance metadata itself (since the utterance is not attached to a corpus):\n","srcMarkdownNoYaml":"\n\n\n\n\nThis notebook demonstrates an application of the Expected Context Framework to analyze Wikipedia talk page discussions. We are working up to the following question: what types of comments are more likely to start conversations that eventually derail into toxic personal attacks, versus conversations that remain civil throughout? The notebook goes over a few key function calls along the way. See [this paper](https://www.cs.cornell.edu/~cristian/Conversations_gone_awry_files/conversations_gone_awry.pdf) for details on the analysis question and datasets, and [this dissertation](https://tisjune.github.io/research/dissertation) for details on the framework and more comments on the below analyses.\n\n\n\nFor this demo, we start by training an Expected Context Model on a collection of Wikipedia discussions (different from the dataset we later analyze). In short, we want to infer different _rhetorical types_ of comments that occur at the start of these discussions. Via the framework, we'll derive representations of comments based on their expected replies (the \"forwards context\" of the conversation), and then infer types of comments by clustering these representations. We interpret the resultant clusters as different rhetorical types.\n\nFor an extended version of this analysis, on a slightly older implementation of the model (using the `PromptTypes` functionality), see [this notebook](https://github.com/CornellNLP/ConvoKit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb).\n\n## 1. Loading and preprocessing the training data\n\nWe represent Wikipedia comments as dependency-parse arcs; in order to capture  rhetorical rather than topical information, we've removed nouns. These features are included with the release of the corpus, and can be loaded as follows:\n\nTo make sure that the arcs are stored in the right format (i.e., as strings, not lists), we apply the following transformer:\n\nIn applying the framework, we need to associate utterances with their replies. One comment can receive multiple replies; here, we arbitrarily choose one (noting that future work could make better use of this additional information). \n\n## 2. Applying the Expected Context Framework\n\n\nTo apply the Expected Context Framework, we start by converting the input utterance text to an input vector representation. Here, we represent utterances in a term-document matrix that's _normalized by columns_ (empirically, we found that this ensures that the representations derived by the framework aren't skewed by the relative frequency of utterances). We use the `ColNormedTfidfTransformer` transformer to do this.\n\nWe found that it was preferable to derive different tf-idf representations (and hence different inverse document frequencies, normalization terms, and vocabularies) for comments that receive replies, and utterances that have predecessors. We make this partitioning by passing in different selectors to the `fit` functions below.\n\nThe two sets are of course overlapping, but the former set mostly contains comments that start conversations, and the second contains more comments that reply to conversation-starters. We note that we'd expect the language of conversation-starters and repliers to be slightly different (i.e., the former tend to be requests, and the latter tend to be responses to these requests).\n\nWe then initialize an Expected Context Model:\n* we specify that the conversational context we will derive our comment representations from is the reply, `context_field=next_id`\n* as input, the model will use tf-idf representations from `vect_field=first_tfidf` to represent comments, and `context_vect_field=second_tfidf` to represent context comments\n* we'll derive `n_svd_dims=25`-dimensional representations, and infer `n_clusters=6` clusters\n* to infer these clusters, we will cluster _term_-level representations, `cluster_on='terms'`. Note that by default, the model will cluster _comment_-level representations, but in this context (perhaps since comments can get fairly long and unstructured), clustering on terms produces more interpretable output.\n\n(note that the following call takes some time to run)\n\n### Inferred comment types\n\nBelow, we print representative terms, comments, and context terms and comments for the clusters we've inferred (note that the output is quite long)\n\ndemo continues below\n\nPer our interpretation, we assign the following names to these clusters:\n\n## 2. Application to analysis data\n\nWe now use the comment types we've just derived to analyze a dataset containing conversations that eventually derail into toxic behavior, and conversations that stay on track throughout. We load this data:\n\nWe start by extracting the same noun-less dependency parse arcs that we used when training the model, above. To do this, we need to load the dependency parses, and then apply a few additional transformers, below:\n\nWe represent the comments from the new dataset as tf-idf vectors, using the vocabulary and parameters we've derived over the training data:\n\nFinally, we apply the trained model to transform the new dataset. In particular, we annotate each comment with an attribute, `fw_clustering.cluster`, that denotes the comment type it's assigned to:\n\nTo facilitate subsequent analyses, we will gather the comment types for each comment into a table:\n\n## 3. Analysis: comparing comment types in awry versus on-track conversations\n\nWe start by preprocessing the data, to facilitate our comparison of awry versus on-track conversations. Ultimately, we will compare the occurrence of comment types in the first and second comments of these discussions.\n\nWe compute log-odds ratios of each comment type, comparing the awry and on-track conversations. We will also compute significance values from binomal tests comparing the proportion of awry-turning conversations exhibiting a particular comment type to the proportion of on-track conversations. \n\nPlotting the resultant log-odds ratios, we note some differences in which types of comments tend to start awry versus on track discussions: _contentious_ comments tend to signal future troubles, while _coordinating_ work is a signal that the conversation will remain civil throughout.\n\n## 4. Pipeline usage\n\nWe can also apply the framework via a pipeline that handles the following:\n* processes text (via a pipeline supplied by the user; see cell below)\n* transforms text to input representation (via `ColNormedTfidfTransformer`)\n* derives framework output (via `ExpectedContextModelTransformer`)\n\nWe initialize the pipeline with the following arguments:\n* `text_field` specifies which utterance metadata field to use as text input.\n* `share_tfidf_models` indicates whether we want to train one `ColNormedTfidfTransformer` model for both utterances and context-utterances. In this case, since we want different input representations for first and second comments, we set this argument to `False`.\n* `text_pipe` specifies the pipeline used to compute the contents of `text_field`\n* `tfidf_params` specifies the parameters to be passed into the underlying `ColNormedTfidfTransformer` object\n* `min_terms` specifies the minimum number of terms in the vocabulary that an utterance must contain for it to be considered in fitting and transforming the underlying `ExpectedContextModelTransformer` object (see the `selector` argument passed into `ec_fw.fit` above)\n\nAll other arguments are inherited from `ExpectedContextModelTransformer`.\n\nThis should produce the same output as calling the constituent steps separately.\n\nThe pipeline enables us to transform ad-hoc string input.\n\nHere, instead of storing vector representations with a corpus, the pipeline writes these representations to a field in the utterance metadata itself (since the utterance is not attached to a corpus):\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"wiki_awry_demo.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","title":"Applying the Expected Context Framework to Wikipedia discussions"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}