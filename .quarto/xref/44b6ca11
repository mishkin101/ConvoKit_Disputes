{"entries":[],"headings":["explanation-of-the-model-and-training-process","model-architecture","utterance-encoder-encoderrnn","this-encoder-processes-individual-utterances-tokenized-sentences.-for-each-utterance-in-a-dialog-it-generates-a-hidden-state-representation.","context-encoder-contextencoderrnn","after-the-utterance-encoder-processes-each-utterance-the-final-hidden-states-of-all-utterances-in-a-dialog-are-combined-into-a-sequence-and-passed-to-the-context-encoder.-this-context-encoder-models-the-conversation-level-context.","luong-attention-decoder-luongattndecoderrnn","the-decoder-uses-luongs-attention-mechanism.-it-takes-the-final-context-representation-as-its-initial-hidden-state-and-either-the-ground-truth-token-teacher-forcing-or-its-own-predicted-token-at-each-time-step-to-produce-the-next-token-in-the-response.","data-flow-during-training","utterance-encoding-each-utterance-in-a-batch-is-passed-through-the-utterance-encoder-which-returns-hidden-states.","context-encoding-the-final-hidden-states-of-all-the-utterances-in-a-dialog-are-packed-together-in-chronological-order-and-passed-to-the-context-encoder-which-returns-a-context-level-representation-of-the-entire-conversation.","decoding-the-decoder-uses-the-final-hidden-state-of-the-context-encoder-to-begin-generating-a-response.","if-teacher-forcing-is-used-a-random-chance-based-on-teacher_forcing_ratio-the-decoder-is-fed-the-ground-truth-token-at-each-step.","otherwise-no-teacher-forcing-the-decoders-own-predicted-token-is-fed-back-as-input-at-the-next-time-step.","masking-and-loss-the-script-calculates-the-negative-log-likelihood-loss-only-over-valid-tokens-excluding-padding-by-applying-a-mask-masknllloss.","backpropagation-and-gradient-clipping-the-loss-is-backpropagated-through-the-entire-network-both-encoders-and-the-decoder.-the-gradients-are-then-clipped-to-avoid-exploding-gradients-and-the-optimizers-step-to-update-the-parameters.","training-loop","batching-batches-of-data-are-constructed-each-containing-several-multi-utterance-dialogs.","forward-pass-the-batch-is-fed-through-the-model-utterance-encoder---context-encoder---decoder.","loss-computation-the-script-accumulates-the-loss-for-each-token-in-the-target-sequence.","backpropagation-the-networks-parameters-are-updated-based-on-the-calculated-gradients.","progress-printing-at-intervals-determined-by-print_every-the-script-prints-the-average-loss.","saving-the-model","the-trained-model-parameters-for-each-component-encoder-context-encoder-decoder-and-the-corresponding-optimizers-are-saved-in-a-dictionary-and-written-to-disk-in-model.tar."]}