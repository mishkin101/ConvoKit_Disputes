{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm\n",
    "\n",
    "from convokit import Corpus, download\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/conversations-gone-awry-corpus\n",
      "Dataset already exists at /Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/conversations-gone-awry-corpus/conversations-gone-awry-cmv-corpus\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/conversations-gone-awry-corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m wiki_path = download(\u001b[33m\"\u001b[39m\u001b[33mconversations-gone-awry-corpus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m cmv_path  = download(\u001b[33m\"\u001b[39m\u001b[33mconversations-gone-awry-cmv-corpus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m corpus_w = \u001b[43mCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwiki_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m corpus_cmv = Corpus(cmv_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/Convo_Kit/ConvoKit_Disputes/.venv/lib/python3.11/site-packages/convokit/model/corpus.py:172\u001b[39m, in \u001b[36mCorpus.__init__\u001b[39m\u001b[34m(self, filename, utterances, db_collection_prefix, db_host, preload_vectors, utterance_start_index, utterance_end_index, merge_lines, exclude_utterance_meta, exclude_conversation_meta, exclude_speaker_meta, exclude_overall_meta, disable_type_check, backend, backend_mapper)\u001b[39m\n\u001b[32m    170\u001b[39m     speakers_data = defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    171\u001b[39m     convos_data = defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     utterances = \u001b[43mload_from_utterance_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance_end_index\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.utterances = \u001b[38;5;28mdict\u001b[39m()\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.speakers = \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/Convo_Kit/ConvoKit_Disputes/.venv/lib/python3.11/site-packages/convokit/model/corpus_helpers.py:313\u001b[39m, in \u001b[36mload_from_utterance_file\u001b[39m\u001b[34m(filename, utterance_start_index, utterance_end_index)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_utterance_file\u001b[39m(filename, utterance_start_index, utterance_end_index):\n\u001b[32m    310\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    where filename is \"utterances.json\" or \"utterances.jsonl\" for example\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    314\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    315\u001b[39m             ext = filename.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/conversations-gone-awry-corpus'"
     ]
    }
   ],
   "source": [
    "# Load the corpus\n",
    "from convokit import Corpus, download\n",
    "\n",
    "\n",
    "wiki_path = download(\"conversations-gone-awry-corpus\", data_dir = \"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets\")\n",
    "cmv_path  = download(\"conversations-gone-awry-cmv-corpus\", data_dir= \"/Users/mishkin/Desktop/Research/Convo_Kit/ConvoKit_Disputes/data/convokit_datasets\")\n",
    "corpus_w = Corpus(wiki_path)\n",
    "corpus_cmv = Corpus(cmv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kodis_corp = corpus_converter.test_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats\n",
    "print(\"Speakers:\", len(list(corpus.iter_speakers())))\n",
    "print(\"Conversations:\", len(list(corpus.iter_conversations())))\n",
    "print(\"Utterances:\", len(list(corpus.iter_utterances())))\n",
    "\n",
    "# Utterances per conversation\n",
    "utterance_counts = [len(list(c.iter_utterances())) for c in corpus.iter_conversations()]\n",
    "plt.hist(utterance_counts, bins=30)\n",
    "plt.title(\"Distribution of Utterances per Conversation\")\n",
    "plt.xlabel(\"Utterances\")\n",
    "plt.ylabel(\"Number of Conversations\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Conversations with removed final comments\n",
    "removed = [c.meta.get(\"has_removed_comment\", False) for c in corpus.iter_conversations()]\n",
    "pd.Series(removed).value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Final Comment Moderation\")\n",
    "plt.xticks([0, 1], [\"Not Removed\", \"Removed\"], rotation=0)\n",
    "plt.ylabel(\"Number of Conversations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of utterances per conversation\n",
    "conversation_lengths = [len(list(convo.iter_utterances())) for convo in corpus.iter_conversations()]\n",
    "\n",
    "# Compute average, median, and max\n",
    "average_length = np.mean(conversation_lengths)\n",
    "median_length = np.median(conversation_lengths)\n",
    "max_length = np.max(conversation_lengths)\n",
    "\n",
    "# Sample a few conversation structures\n",
    "sample_structures = {}\n",
    "for i, convo in enumerate(corpus.iter_conversations()):\n",
    "    if i >= 3: break  # Just look at 3 examples\n",
    "    utt_structure = [(utt.id, utt.reply_to) for utt in convo.iter_utterances()]\n",
    "    sample_structures[convo.id] = utt_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(average_length, median_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of conversations by number of unique speakers\n",
    "speaker_count_distribution = defaultdict(int)\n",
    "\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    speaker_ids = {utt.speaker.id for utt in convo.iter_utterances()}\n",
    "    speaker_count_distribution[len(speaker_ids)] += 1\n",
    "\n",
    "# Sort for plotting\n",
    "sorted_counts = dict(sorted(speaker_count_distribution.items()))\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_counts.keys(), sorted_counts.values(), color=\"blue\")\n",
    "plt.xlabel(\"Number of Unique Speakers in Conversation\")\n",
    "plt.ylabel(\"Number of Conversations\")\n",
    "plt.title(\"Distribution of Conversations by Number of Unique Speakers\")\n",
    "plt.xticks(list(sorted_counts.keys()))\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# List to store lengths of derailed conversations\n",
    "derailed_lengths = []\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta.get(\"has_removed_comment\") == True:\n",
    "        num_utts = len(list(convo.iter_utterances()))\n",
    "        derailed_lengths.append(num_utts)\n",
    "\n",
    "# Compute average length before derailment\n",
    "average_derailment_length = np.mean(derailed_lengths)\n",
    "\n",
    "# Plot histogram of lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(derailed_lengths, bins=range(1, max(derailed_lengths)+1), color=\"tomato\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Distribution of Conversation Lengths Before Derailment (CGA-CMV)\")\n",
    "plt.xlabel(\"Number of Utterances\")\n",
    "plt.ylabel(\"Number of Derailed Conversations\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "average_derailment_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from convokit import Corpus, download\n",
    "\n",
    "\n",
    "\n",
    "# Group all corpora\n",
    "corpora = {\n",
    "    \"CGA-CMV\": cmv_corpus,\n",
    "    \"CGA-WIKI\": wiki_corpus,\n",
    "    \"Test Corpus\": test_corp\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(len(corpora), 4, figsize=(20, 12))\n",
    "fig.suptitle(\"Conversational Characteristics Across Corpora\", fontsize=16)\n",
    "\n",
    "for i, (label, corpus) in enumerate(corpora.items()):\n",
    "    # --- Plot 1: Distribution of Utterances per Conversation ---\n",
    "    utterance_counts = [len(list(c.iter_utterances())) for c in corpus.iter_conversations()]\n",
    "    axs[i, 0].hist(utterance_counts, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "    axs[i, 0].set_title(f\"{label}\\nUtterances per Conversation\")\n",
    "    axs[i, 0].set_xlabel(\"Utterances\")\n",
    "    axs[i, 0].set_ylabel(\"Conversations\")\n",
    "\n",
    "    # --- Plot 2: Final Comment Moderation ---\n",
    "    removed = [c.meta.get(\"has_removed_comment\", False) for c in corpus.iter_conversations()]\n",
    "    pd.Series(removed).value_counts().sort_index().plot(kind=\"bar\", ax=axs[i, 1], color=[\"gray\", \"red\"])\n",
    "    axs[i, 1].set_title(f\"{label}\\nFinal Comment Moderation\")\n",
    "    axs[i, 1].set_xticks([0, 1])\n",
    "    axs[i, 1].set_xticklabels([\"Not Removed\", \"Removed\"])\n",
    "    axs[i, 1].set_ylabel(\"Conversations\")\n",
    "\n",
    "    # --- Plot 3: Unique Speakers per Conversation ---\n",
    "    speaker_count_distribution = defaultdict(int)\n",
    "    for convo in corpus.iter_conversations():\n",
    "        speaker_ids = {utt.speaker.id for utt in convo.iter_utterances()}\n",
    "        speaker_count_distribution[len(speaker_ids)] += 1\n",
    "    sorted_counts = dict(sorted(speaker_count_distribution.items()))\n",
    "    axs[i, 2].bar(sorted_counts.keys(), sorted_counts.values(), color=\"blue\")\n",
    "    axs[i, 2].set_title(f\"{label}\\nSpeakers per Conversation\")\n",
    "    axs[i, 2].set_xlabel(\"Unique Speakers\")\n",
    "    axs[i, 2].set_ylabel(\"Conversations\")\n",
    "\n",
    "    # --- Plot 4: Lengths of Derailed Conversations ---\n",
    "    derailed_lengths = [\n",
    "        len(list(convo.iter_utterances()))\n",
    "        for convo in corpus.iter_conversations()\n",
    "        if convo.meta.get(\"has_removed_comment\") == True\n",
    "    ]\n",
    "    axs[i, 3].hist(\n",
    "        derailed_lengths,\n",
    "        bins=range(1, max(derailed_lengths) + 1 if derailed_lengths else 2),\n",
    "        color=\"tomato\",\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "    axs[i, 3].set_title(f\"{label}\\nLength Before Derailment\")\n",
    "    axs[i, 3].set_xlabel(\"Utterances\")\n",
    "    axs[i, 3].set_ylabel(\"Derailed Conversations\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
